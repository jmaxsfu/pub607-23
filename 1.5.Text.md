# A History of Digitized Text

What's so special about text?

It *matters* what kind of stuff is in the file. A binary file (like an image, or audio, or an InDesign document) is, fundamentally hard to edit. It needs to be reconstituted by some application into a workable format -- for instance, in an audio editing application -- then saved back into the file format.

Text files -- and text-based data -- on the other hand, are editable as they are, because their structure corresponds exactly (or at least closely) to what they represent. A plain text file is made up exactly of the letters you read when you read it. Editing the file is identical with editing the text.

Text is special. This goes beyond the mechanics of files and file formats. Text is special because it's uniquely editable, changeable, mutable. What's that? You thought it was special because it was unchanging! But that's *print*, not *text*. The "text" comes into being at the point when Gutenberg makes the type "moveable."

Text can be edited; it can be parsed; it can be processed in ways that are incredibly broadly understood, literally by every literate person. Compare that to all that's required to edit two seconds out of a video file -- or your sister's ex from a family photo.

(Actually, when we have systems that *generate* audio or video, we typically interact with it via 'notation,' which is intended to work sort of like text. Think about music scores, for instance. And XML, more about which below...)

Text is special because we know how to process it, and so making computers process text is an extension of our own understanding, as opposed to something unique that has to be invented (like Photoshop or something).

## A tale of two paradigms

Interestingly, the history of how we think about text in computers has two parallel traditions, very different in approach. These intersect here and there, but are almost two distinct *paradigms*.

The first tradition starts with text files, and builds structure into these files in order to deal with things like formatting, metadata, and so on. This tradition comes out of old-school typesetting and leads through Standard Generalized Markup Language (SGML, from the 1980s), HyperText Markup Language (HTML, early 1990s), to eXtensible Markup Language (XML, from the late 1990s). It is the approach that underlies the Web and most "digital publishing" formats.

In this way of thinking, the text and its innate structures are of paramount importance, and things like formatting and layout are secondary; they need to be layered onto the underlying text structures. This approach is extremely well suited to accessibility, as a single text can be manifest in different formats for different audiences or different contexts.

The second tradition, largely invented at Xerox PARC in the 1970s, makes page layout primary, and everything else is secondary. This tradition begins with "Desktop Publishing" (DTP) leading through word processors and software like PageMaker, QuarkXpress, and InDesign, and reaches its full express in the PDF -- which is a perfect virtual image of a page. The goal is "WYSIWYG" -- *what you see is what you get* -- that what you see on the computer screen is what you'll get on the printed page. This tradition has been dominant in publishing since around 1990, and it utterly replaced traditional typesetting. 
